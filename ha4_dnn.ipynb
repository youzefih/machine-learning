{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS596 Machine Learning \n",
    "# Homework Assignment 4 (Part 2): Deep Neural Network\n",
    "\n",
    "### Due 11:59 pm, Friday, 10/12/2018\n",
    "\n",
    "**Total credits: 8.5**\n",
    "\n",
    "In Part 2 of HA4, we will implement a 2-layer shallow neural network model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from utils import *\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks of DNN\n",
    "\n",
    "The basic building unit of DNN is described in the following figure.\n",
    "\n",
    "<img src=\"dnn_building_block.pdf\" style=\"width: 360px\">\n",
    "\n",
    "Combing multiple units together, the computation graph of a DNN will be as follows:\n",
    "\n",
    "<img src=\"dnn_building_block2.pdf\">\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### Decomposing forward block\n",
    "\n",
    "The `forward` block of layer $l$ can be decomposed into two sub-modules: `linear_forward` and `activation_forward`\n",
    "- `linear_forward` computes $Z^{[l]} = W^{[l]}A^{[l-1]}+b^{[l]}$, and then caches $A^{[l-1]}$ into `linear_cache`. For convenience, `linear_cache` also stores $W^{[l]}$ and $b^{[l]}$.\n",
    "- `activation_forward` computes $A^{[l]} = g^{[l]}(Z^{[l]})$, and then caches $Z^{[l]}$ into `activation_cache`\n",
    "- The `forward` unit outputs $A^{[l]}$, and combines `linear_cache` and `activation_cache` into `cache`\n",
    "\n",
    "<img src=\"forward_decompose.pdf\">\n",
    "\n",
    "***\n",
    "\n",
    "### 2.1 Implement linear_forward\n",
    "**0.5 credit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    A_prev -- activations from previous layer, of shape (size of previous layer, m)\n",
    "    W -- weight matrix of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector of shape (size of current layer, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- output of the linear forward computation, and the input fed into the next activation forward module\n",
    "    cache -- a python dict object containing all cached variables, i.e., A_prev, W, and b\n",
    "    \"\"\"\n",
    "    ### START TODO ###\n",
    "    Z = np.dot(W,A_prev) + b\n",
    "    ### END TODO ###\n",
    "    \n",
    "    linear_cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, linear_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 0.04347969 -0.01180377]\n",
      " [-0.04910105 -0.00973779]\n",
      " [ 0.0140403  -0.02289789]\n",
      " [-0.01817968 -0.00986889]\n",
      " [ 0.01867231  0.00796029]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 2.1\n",
    "A, W, b = utils.linear_forward_testcase()\n",
    "\n",
    "Z, _ = linear_forward(A, W, b)\n",
    "print('Z =', Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**Z =**|[[ 0.04347969 -0.01180377]<br>[-0.04910105 -0.00973779]<br>[ 0.0140403  -0.02289789]<br>[-0.01817968 -0.00986889]<br>[ 0.01867231  0.00796029]]|\n",
    "\n",
    "***\n",
    "\n",
    "### 2.2 Implement activation_forward\n",
    "**1 credit**\n",
    "\n",
    "Instructions:\n",
    "- The `func` argument of `activation_forward` denotes which activation function to use. You need to implement `sigmoid_forward` and `relu_forward` respectively, and then call them in `activation_forward`.\n",
    "- `sigmoid_forward` is a bit different from the pre-defined `sigmoid` function in `utils`, because you need to cache $Z$ and return it together with the activation\n",
    "- $ReLU(Z)=\\begin{cases}Z, & \\text{if } Z>0.\\\\ 0, & \\text{otherwise.} \\end{cases}$  You can implement it using `np.maximum` function\n",
    "- For either activation function, `cache` is just its input $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_forward(Z):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Z -- a numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(Z), same shape as Z\n",
    "    cache -- Z\n",
    "    \"\"\"\n",
    "    ### START TODO ###\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    activation_cache = Z\n",
    "    ### END TODO ###\n",
    "    \n",
    "    return A, activation_cache\n",
    "\n",
    "def relu_forward(Z):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Z -- a numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(Z), same shape as Z\n",
    "    cache -- Z\n",
    "    \"\"\"\n",
    "    ### START TODO ###\n",
    "    A = np.maximum(0,Z)\n",
    "    activation_cache = Z \n",
    "    ### END TODO ###\n",
    "    \n",
    "    return A, activation_cache\n",
    "\n",
    "def activation_forward(Z, func):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Z -- a numpy array of any shape\n",
    "    func -- a string of either 'sigmoid' or 'relu'\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the activation function\n",
    "    cache -- cache returned by the activation function\n",
    "    \"\"\"\n",
    "    assert func in ['sigmoid', 'relu']\n",
    "    \n",
    "    if func == 'sigmoid': \n",
    "        ### START TODO ###\n",
    "        A, activation_cache = sigmoid_forward(Z) # Call sigmoid_forward\n",
    "        ### END TODO ###\n",
    "    elif func == 'relu':\n",
    "        ### START TODO ###\n",
    "        A, activation_cache = relu_forward(Z) # Call sigmoid_forward\n",
    "        ### END TODO ###\n",
    "    \n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid function, A = [0.73105858 0.26894142 0.5       ]\n",
      "With sigmoid function, A = [1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 2.2\n",
    "Z = utils.activation_forwad_testcase()\n",
    "\n",
    "A, _ = activation_forward(Z, func='sigmoid')\n",
    "print('With sigmoid function, A =', A)\n",
    "A, _ = activation_forward(Z, func='relu')\n",
    "print('With sigmoid function, A =', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**With sigmoid function, A =**|[0.73105858 0.26894142 0.5       ]|\n",
    "|**With sigmoid function, A =**|[1 0 0]|\n",
    "\n",
    "***\n",
    "\n",
    "### 2.3 Implement forward\n",
    "**0.5 credits**\n",
    "\n",
    "In `forward`, call `linear_forward` and `activation_forward` in order, and return the activation value and the combined cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(A_prev, W, b, func):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    A_prev -- activation from previous layer\n",
    "    W -- Weight matrix\n",
    "    b -- bias vector\n",
    "    func -- 'sigmoid' or 'relu'\n",
    "    \n",
    "    Returns:\n",
    "    A -- activation\n",
    "    cache -- tuple of (linear_cache, activation_cache)\n",
    "    \"\"\"\n",
    "    # Compute output of linear_forward and activation_forward\n",
    "    ### START TODO ###\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if func ==\"sigmoid\":\n",
    "        A, activation_cache = sigmoid_forward(Z)\n",
    "    elif func ==\"relu\":\n",
    "        A, activation_cache = relu_forward(Z)\n",
    "    ### END TODO ###\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid function, A = [[0.51086821 0.49704909]\n",
      " [0.4877272  0.49756557]\n",
      " [0.50351002 0.49427578]\n",
      " [0.4954552  0.4975328 ]\n",
      " [0.50466794 0.50199006]]\n",
      "With relu function, A = [[0.04347969 0.        ]\n",
      " [0.         0.        ]\n",
      " [0.0140403  0.        ]\n",
      " [0.         0.        ]\n",
      " [0.01867231 0.00796029]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 2.3\n",
    "A_prev, W, b = utils.forward_testcase()\n",
    "\n",
    "A, _ = forward(A_prev, W, b, func='sigmoid')\n",
    "print('With sigmoid function, A =', A)\n",
    "A, _ = forward(A_prev, W, b, func='relu')\n",
    "print('With relu function, A =', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**With sigmoid function, A =**|[[0.51086821 0.49704909]<br>[0.4877272  0.49756557]<br>[0.50351002 0.49427578]<br>[0.4954552  0.4975328 ]<br>[0.50466794 0.50199006]]|\n",
    "|**With sigmoid function, A =**|[[0.04347969 0.        ]<br>[0.         0.        ]<br>[0.0140403  0.        ]<br>[0.         0.        ]<br>[0.01867231 0.00796029]]|\n",
    "\n",
    "***\n",
    "\n",
    "### 2.4 Initialize parameters for DNN\n",
    "**1 credit**\n",
    "\n",
    "The number of layers and layer sizes are specified by a Python list `layer_sizes`. For example, if `layer_sizes = [4096, 128, 64, 10, 1]`, then the model has **4** layers (The first element 4096 is the size of input, which is not counted as a layer).\n",
    "`layer_sizes[1]` is the number of hidden units in layer 1, `layer_sizes[2]` is for layer 2, and so on.\n",
    "\n",
    "Therfore, the shapes of parameters associated with each layer are: \n",
    "- $W^{[1]}: (128, 4096)\\quad b^{[1]}: (128,1)$\n",
    "- $W^{[2]}: (64, 128)\\quad b^{[2]}: (64,1)$\n",
    "- $W^{[3]}: (10, 64)\\quad b^{[3]}: (10,1)$\n",
    "- $W^{[4]}: (1, 10)\\quad b^{[4]}: (1,1)$\n",
    "\n",
    "In general, if the length of `layer_sizes` is $L$, then `layer_sizes[0]` is the input size, and `layer_sizes[1]` through `layer_sizes[L-2]` indicate the sizes of hidden layers. `layer_sizes[L-1]` is the size of output layer.\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "- Because all the hidden layers use ReLU activation, the proper way to initialize their weights is to use:<br>\n",
    "$W^{[l]}=\\text{np.random.randn}(n^{[l]}, n^{[l-1]})*\\text{np.sqrt}(\\frac{2}{n^{[l-1]}})$, where $l=1,2,\\dots,L-2$\n",
    "\n",
    "- For the output layer, the proper way to initialize its weight is:<br>\n",
    "$W^{[L-1]}=\\text{np.random.randn}(n^{[L-1]}, n^{[L-2]})*\\text{np.sqrt}(\\frac{1}{n^{[l-1]}})$\n",
    "\n",
    "- All bias vectors $b^{[l]}$ are initialized to 0s.\n",
    "\n",
    "- Parameters are stored in a python dict object, whose keys are strings like `'W1', 'b1', 'W2', 'b2'` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params_deep(layer_sizes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    layer_sizes -- a python list containing the sizes of input layer, hidden layers, and output layer.\n",
    "    \n",
    "    Returns:\n",
    "    params -- a python dict object containing all initialized parameters, i.e., 'W1', 'b1', 'W2', 'b2' etc.\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(layer_sizes)\n",
    "    assert(L >= 2)\n",
    "    # Initialize all L-2 hidden layers\n",
    "    for l in range(1, L-1):\n",
    "        ### START TODO ###\n",
    "        params['W' + str(l)] = np.random.randn(layer_sizes[l], layer_sizes[l-1]) * np.sqrt(2/layer_sizes[l-1])\n",
    "        params['b' + str(l)] = np.zeros((layer_sizes[l], 1))\n",
    "        ### END TODO ###\n",
    "    \n",
    "    # Initialize output layer\n",
    "    ### START TODO ###\n",
    "    params['W' + str(L-1)]=np.random.randn(layer_sizes[l],1) * np.sqrt(1/layer_sizes[l]) \n",
    "    params['b' + str(L-1)] = np.zeros((layer_sizes[l+1],1)) \n",
    "    ### END TODO ###\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.13122797  0.27607307  0.06103036 -1.17857627 -0.1754357 ]\n",
      " [-0.22436928 -0.05233031 -0.39655005 -0.02771304 -0.30181918]\n",
      " [-0.83096103  0.55948432  0.55739447  1.08122894  0.03164405]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.23364061]\n",
      " [-0.31486371]\n",
      " [-0.89285909]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 2.4\n",
    "params = init_params_deep([5,3,1])\n",
    "\n",
    "print('W1 =', params['W1'])\n",
    "print('b1 =', params['b1'])\n",
    "print('W2 =', params['W2'])\n",
    "print('b2 =', params['b2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**W1 =**|[[ 1.13122797  0.27607307  0.06103036 -1.17857627 -0.1754357 ]<br>[-0.22436928 -0.05233031 -0.39655005 -0.02771304 -0.30181918]<br>[-0.83096103  0.55948432  0.55739447  1.08122894  0.03164405]]|\n",
    "|**b1 =**|[[0.]<br>[0.]<br>[0.]]|\n",
    "|**W2 =**|[[-0.23364061 -0.31486371 -0.89285909]]|\n",
    "|**b2 =**|[[0.]]|\n",
    "\n",
    "***\n",
    "\n",
    "### 2.5 Implement forward_deep\n",
    "**1.5 credits**\n",
    "\n",
    "For a $L$-layer neural network, the whole forward propagation runs the previously defined `forward` with `func='relu'` $L-1$ times, and follows with `forward` with `func='sigmoid'` (see the figure below).\n",
    "\n",
    "<img src='forward_deep.pdf'>\n",
    "\n",
    "Instructions:\n",
    "- `forward_deep` takes input data $X$ and `params` as arguments, and compute $L$ steps of forward propagation using the $W^{[l]}$ and $b^{[l]}$ stored in `params`.\n",
    "- It collects the `cache` from each layer and store them in a python list `caches`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_deep(X, params):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- input data of shape (input_size, m)\n",
    "    params -- output of init_params_deep\n",
    "    \n",
    "    W -- weight matrix of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector of shape (size of current layer, 1)\n",
    "    linear_forward(A_prev, W, b):\n",
    "    \n",
    "    Returns:\n",
    "    AL -- the activation of the output layer\n",
    "    caches -- a python list containing the cache object for each layer\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A_prev = X\n",
    "    L = len(params) // 2 # Because each layer has 2 parametes, this will give the number of layers\n",
    "    \n",
    "    # Forward propagation for the first L-1 relu layers\n",
    "    for l in range(1, L):\n",
    "        ### START TODO ###\n",
    "        # Retrieve parameters\n",
    "        W = params['W'+str(l)]\n",
    "        b = params['b' + str(l)]\n",
    "        \n",
    "        # Call forward function, with 'relu'\n",
    "        A, cache =  forward(A_prev, W,b,\"relu\")\n",
    "        \n",
    "        ### END TODO ###\n",
    "        \n",
    "        caches.append(cache)\n",
    "        A_prev = A\n",
    "    \n",
    "    # Forward propagation for the last sigmoid output layer\n",
    "    ### START TODO ###\n",
    "    # Retrieve parameters\n",
    "    W = params['W'+str(L)]\n",
    "    b = params['b' + str(L)]\n",
    "    \n",
    "    # Call forward function, with 'sigmoid'\n",
    "    AL, cache = forward(A_prev, W,b,\"sigmoid\")\n",
    "    ### END TODO ###\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL.shape =  (1, 100)\n",
      "mean of AL = 0.4999999588605858\n",
      "Length of caches = 5\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 2.5\n",
    "X, params = utils.forward_deep_testcase()\n",
    "\n",
    "AL, caches = forward_deep(X, params)\n",
    "print('AL.shape = ', AL.shape)\n",
    "print('mean of AL =', np.mean(AL))\n",
    "print('Length of caches =', len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp;|          \n",
    "|--|--|\n",
    "|**AL.shape =**|(1, 100)|\n",
    "|**mean of AL =**|0.4999999588605858|\n",
    "|**Length of caches =**|5|\n",
    "\n",
    "***\n",
    "\n",
    "### Decomposing backward block\n",
    "\n",
    "The `backward` block of layer $l$ can be decomposed into two sub-modules as well: `activation_backward` and `linear_backward`.\n",
    "\n",
    "- `activation_backward` takes $dA^{[l]}$ as input to compute $dZ^{[l]}$, and then passes it to `linear_backward`.\n",
    "- `linear_backward` takes $dZ^{[l]}$ to compute $dW^{[l]}$, $db^{[l]}$, and $dA^{[l-1]}$, and passes the latter to the previous layer.\n",
    "\n",
    "Both modules need $\\text{cache}^{[l]}$ to do the computation. $\\text{cache}^{[l]} = (\\text{linear_cache}^{[l]}, \\text{activation_cache}^{[l]})$\n",
    "- `activation_backward` uses $\\text{activation_cache}^{[l]})$\n",
    "- `linear_backward` uses $\\text{linear_cache}^{[l]})$\n",
    "\n",
    "See the figure below.\n",
    "<img src='backward_decompose.pdf'>\n",
    "\n",
    "***\n",
    "\n",
    "### 2.6 Implement activation_backward\n",
    "**0.5 credit**\n",
    "\n",
    "The formula of computing $dZ^{[l]}$ is: $dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$\n",
    "- For sigmoid function, $g'(z) = g(z)(1-g(z))$, therefore, $g'(Z^{[l]}) = A^{[l]}(1 - A^{[l]})$\n",
    "- For ReLU function, $g'(z) = \\begin{cases}1, & \\text{if } Z>0.\\\\ 0, & \\text{otherwise.} \\end{cases}$\n",
    "\n",
    "Instructions:\n",
    "- You only need to implement `sigmoid_backward`. `relu_backward` is implemented for you.\n",
    "- The `activation_cache` argument is just the `Z` you need.\n",
    "- `activation_backward` wraps around the above two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dA -- gradient passed from next layer\n",
    "    activation_cache -- 'Z',  which is stored during forward propagation\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost w.r.t. Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    \n",
    "    \n",
    "    ### START TODO ###\n",
    "    A=1/(1+np.exp(-Z))\n",
    "    dZ = dA * A * (1-A)\n",
    "    ### END TODO ###\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dA -- gradient passed from next layer\n",
    "    activation_cache -- 'Z',  which is stored during forward propagation\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost w.r.t. Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    # Set dZ to 0 when Z <= 0. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def activation_backward(dA, activation_cache, func):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dA -- gradient passed from next layer\n",
    "    activation_cache -- 'Z',  which is stored during forward propagation\n",
    "    func -- 'sigmoid' or 'relu', indicates the activation function\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost w.r.t. Z\n",
    "    \"\"\"\n",
    "    ### START TODO ###\n",
    "    if func == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    elif func == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    ### END TODO ###\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid, dZ = [[-0.10414453 -0.01044791 -0.49705693]\n",
      " [ 0.3756869  -0.44831788 -0.15174916]\n",
      " [ 0.10965887 -0.31131568 -0.21940518]\n",
      " [-0.22586725  0.13561934  0.45300721]]\n",
      "With relu, dZ = [[-0.41675785  0.         -2.1361961 ]\n",
      " [ 0.          0.         -0.84174737]\n",
      " [ 0.         -1.24528809  0.        ]\n",
      " [ 0.          0.55145404  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 2.6\n",
    "dA, activation_cache = utils.activation_backward_testcase()\n",
    "\n",
    "dZ = activation_backward(dA, activation_cache, func='sigmoid')\n",
    "print('With sigmoid, dZ =', dZ)\n",
    "dZ = activation_backward(dA, activation_cache, func='relu')\n",
    "print('With relu, dZ =', dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|**With sigmoid, dZ =**|[[-0.10414453 -0.01044791 -0.49705693]<br>[ 0.3756869  -0.44831788 -0.15174916]<br>[ 0.10965887 -0.31131568 -0.21940518]<br>[-0.22586725  0.13561934  0.45300721]]|\n",
    "|**With relu, dZ =**|[[-0.41675785  0.         -2.1361961 ]<br>[ 0.          0.         -0.84174737]<br>[ 0.         -1.24528809  0.        ]<br>[ 0.          0.55145404  0.        ]]|\n",
    "\n",
    "***\n",
    "\n",
    "### 2.7 Implement linear_backward\n",
    "**1 credit**\n",
    "\n",
    "Use the following formulas:\n",
    "- $dW^{[l]} = \\frac{1}{m}dZ^{[l]}A^{[l-1]T}$\n",
    "- $db^{[l]} = \\frac{1}{m}\\text{np.sum}(dZ, \\text{axis}=1, \\text{keepdims}=\\text{True})$\n",
    "- $dA^{[l-1]} = W^{[l]T}dZ^{[l]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dZ -- Gradient passed from activation_backward\n",
    "    linear_cache -- the tuple of (A_prev, W, b), stored during forward propagation\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient to pass to previous layer (layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of W (current layer l), same shape as W\n",
    "    db -- Gradient of b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = linear_cache # Retrieve cached variables\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    ### START TODO ###\n",
    "    dW = 1./m*np.dot(dZ, A_prev.T)\n",
    "    db = 1./m*np.sum(dZ, axis = 1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    ### END TODO ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[-0.6916374   1.1105614   1.2886543 ]\n",
      " [-0.54290127  1.38111692  0.61709641]]\n",
      "dW = [[-0.36864801 -0.75350079]\n",
      " [ 0.53977074 -0.64420274]\n",
      " [ 0.28091023 -0.5063566 ]\n",
      " [ 0.19379656  1.07490378]]\n",
      "db = [[-0.86974026]\n",
      " [-0.33163738]\n",
      " [-0.60011963]\n",
      " [ 0.64488481]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 2.7\n",
    "dZ, linear_cache = utils.linear_backward_testcase()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print('dA_prev =', dA_prev)\n",
    "print('dW =', dW)\n",
    "print('db =', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp;|          \n",
    "|--|--|\n",
    "|**dA_prev =**|[[-0.6916374   1.1105614   1.2886543 ]<br>[-0.54290127  1.38111692  0.61709641]]|\n",
    "|**dW =**|[[-0.36864801 -0.75350079]<br>[ 0.53977074 -0.64420274]<br>[ 0.28091023 -0.5063566 ]<br>[ 0.19379656  1.07490378]]|\n",
    "|**db =**|[[-0.86974026]<br>[-0.33163738]<br>[-0.60011963]<br>[ 0.64488481]]|\n",
    "\n",
    "***\n",
    "\n",
    "### 2.8 Impliment backward\n",
    "**0.5 credit**\n",
    "\n",
    "In `backward`, call `activation_backward` and `linear_backward` in order, and return gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(dA, cache, func):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    dA -- gradient passed from next layer\n",
    "    cache -- a tuple of (linear_cache, activation_cache)\n",
    "    func -- 'sigmoid' or 'relu', indicates the activation function\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient to pass to previous layer (layer l-1)\n",
    "    dW -- Gradient of W (current layer l)\n",
    "    db -- Gradient of b (current layer l)\n",
    "    \"\"\"\n",
    "    # Unpack cache\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    ### START TODO ###\n",
    "    if func ==\"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    elif func ==\"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    ### END TODO ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid, dA_prev = [[-0.14734295  0.27565994  0.2952049 ]\n",
      " [-0.11479235  0.34582924  0.12920287]]\n",
      "With relu, dA_prev = [[ 0.31168109 -0.50634887  2.33674401]\n",
      " [-0.00376134  1.10107013  0.11239834]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 2.8\n",
    "dA, cache = utils.backward_testcase()\n",
    "\n",
    "dA_prev, _, _ = backward(dA, cache, func='sigmoid')\n",
    "print('With sigmoid, dA_prev =', dA_prev)\n",
    "dA_prev, _, _ = backward(dA, cache, func='relu')\n",
    "print('With relu, dA_prev =', dA_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp;|          \n",
    "|--|--|\n",
    "|**With sigmoid, dA_prev =**|[[-0.14734295  0.27565994  0.2952049 ]<br>[-0.11479235  0.34582924  0.12920287]]|\n",
    "|**With relu, dA_prev =**|[[ 0.31168109 -0.50634887  2.33674401]<br>[-0.00376134  1.10107013  0.11239834]]|\n",
    "\n",
    "***\n",
    "\n",
    "### 2.9 Implement backward_deep\n",
    "**2 credit**\n",
    "\n",
    "Backward propagation starts from the output layer. \n",
    "\n",
    "The loss function for binary classification is: $L = -(Y\\log(A^{[L]}) + (1-Y)\\log(1 - A^{[L]}))$\n",
    "\n",
    "Take its derivative w.r.t $A^{[L]}$ results in: $dA^{[L]} = - (\\frac{Y}{A^{[L]}} - \\frac{1-Y}{1-A^{[L]}})$.\n",
    "\n",
    "Then you compute $dA^{[L-1]}$, $dW^{[L]}$, and $db^{[L]}$ by calling `backward` function with `func='sigmoid'`.\n",
    "- You store all the gradients to a python dict object `grads`.\n",
    "\n",
    "After this step, for $l$ from $L-1$ to $1$, you compute $dA^{[l-1]}$, $dW^{[l]}$, and $db^{[l]}$ by calling `backward` function with `func='relu'`\n",
    "- In step $l$, when you need $dA^{[l]}$ as input, you can access it by `grads['dA' + str(l)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_deep(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    # Compute dAL\n",
    "    ### START TODO ### \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END TODO ###\n",
    "    \n",
    "    # Lth layer, Input: \"AL, Y, caches\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START TODO ### \n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp =backward(dAL, current_cache,\"sigmoid\")\n",
    "    \n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    ### END TODO ###\n",
    "    \n",
    "    # For layer L-1 to layer 1\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START TODO ###\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward(dA_prev_temp,  current_cache, \"relu\")\n",
    "        \n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END TODO ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Task 2.9\n",
    "AL, Y, caches = utils.backward_deep_testcase()\n",
    "\n",
    "grads = backward_deep(AL, Y, caches)\n",
    "\n",
    "# print(grads)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "|&nbsp;|&nbsp;|          \n",
    "|--|--|\n",
    "|**dW1 =**|[[0.41010002 0.07807203 0.13798444 0.10502167]<br>[0.         0.         0.         0.        ]<br>[0.05283652 0.01005865 0.01777766 0.0135308 ]]|\n",
    "|**db1 =**|[[-0.22007063]<br>[ 0.        ]<br>[-0.02835349]]|\n",
    "|**dA1 =**|[[ 0.12913162 -0.44014127]<br>[-0.14175655  0.48317296]<br>[ 0.01663708 -0.05670698]]|\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
